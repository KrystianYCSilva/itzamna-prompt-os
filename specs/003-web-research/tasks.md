# Implementation Tasks: WEB-RESEARCH Protocol Enhancement

**Feature**: WEB-RESEARCH Protocol Enhancement (SPEC-003)  
**Spec**: `C:\Users\kryst\IdeaProjects\itzamna-prompt-os\specs\003-web-research\spec.md`  
**Plan**: `C:\Users\kryst\IdeaProjects\itzamna-prompt-os\specs\003-web-research\plan.md`  
**Research**: `C:\Users\kryst\IdeaProjects\itzamna-prompt-os\specs\003-web-research\research.md`  
**Created**: 2026-02-03  
**Status**: Ready for Implementation

---

## Overview

This feature enhances the WEB-RESEARCH protocol with formal source validation rules, structured citation templates, quality tier scoring, and AUTO-INCREMENT integration. The implementation is **protocol-based** (Markdown documentation) rather than code development.

**Key Facts**:
- **Type**: Protocol enhancement (Markdown files)
- **User Stories**: 5 (US1-US5, priorities P1-P3)
- **Technology**: Markdown-based prompt engineering
- **Architecture**: JIT sub-files pattern (main protocol + 4 sub-files)
- **Token Budget**: Main protocol <1,100 tokens (T0-SIZE-01 compliance)
- **Integration**: AUTO-INCREMENT, MEMORY-MANAGEMENT, SELF-CRITIQUE, HUMAN-GATE

---

## Task Summary

**Total Tasks**: 23  
**Parallelizable Tasks**: 6  
**Phases**: 7 (Setup + Foundational + 5 User Stories)

| Phase | User Story | Priority | Tasks | Independent Test |
|-------|------------|----------|-------|------------------|
| 1 | Setup | - | 2 | N/A (infrastructure) |
| 2 | Foundational | - | 0 | N/A (no blocking prerequisites) |
| 3 | US1: Source Validation | P1 | 5 | Agent validates 10 sources, scores consistent Â±5 points |
| 4 | US2: Citation Templates | P1 | 4 | 3 test skills use correct templates (minimal/standard/detailed) |
| 5 | US3: Tier Classification | P2 | 4 | 10 sources correctly assigned to T1-T5 tiers |
| 6 | US4: Gap Detection | P2 | 4 | 4 gap scenarios trigger AUTO-INCREMENT correctly |
| 7 | US5: JIT Refactoring | P3 | 3 | Main protocol <1,400 tokens, JIT sub-files load correctly |
| 8 | Polish | - | 1 | Cross-cutting concerns addressed |

**Estimated Effort**: 8.5 hours implementation + 1 hour documentation + 0.5 hour memory = **10 hours total** (~1.5 days)

---

## Dependencies & Execution Strategy

### User Story Completion Order

```
Setup Phase (Phase 1)
    â†“
US1 (Phase 3) - Source Validation [P1] â† Foundation for all other stories
    â†“
US2 (Phase 4) - Citation Templates [P1] â† Depends on US1 validation rules
    â†“
US3 (Phase 5) - Tier Classification [P2] â† Depends on US1 scoring system
    â†“
US4 (Phase 6) - Gap Detection [P2] â† Depends on US1 validation + US3 tiers
    â†“
US5 (Phase 7) - JIT Refactoring [P3] â† Depends on US1-US4 (all sub-files must exist)
    â†“
Polish (Phase 8)
```

**Critical Path**: US1 â†’ US2 â†’ US3 â†’ US4 â†’ US5 (sequential, each builds on previous)

**No Parallel Opportunities Between Stories**: Due to architectural dependencies (US1 defines scoring used by US3, US3 defines tiers used by US4, etc.)

### Parallel Opportunities Within Stories

**US1 (Phase 3)**: Tasks T004-T005 parallelizable after T003 complete  
**US2 (Phase 4)**: Tasks T009-T010 parallelizable after T008 complete  
**US3 (Phase 5)**: Tasks T013-T014 parallelizable after T012 complete  
**US4 (Phase 6)**: All tasks sequential (integration dependencies)

---

## MVP Scope Recommendation

**MVP = User Story 1 (US1) Only**

**Rationale**:
- US1 provides core validation capability (objective scoring system)
- US1 is independently testable (provide 10 URLs, verify scores)
- US1 delivers immediate value (consistent source quality assessment)
- US1 unblocks all other stories (US2-US5 depend on US1 foundation)

**MVP Deliverable**: `source-validation-rules.md` with 4-dimension scoring rubric

**Post-MVP Increments**:
- **Increment 2**: US2 (Citation Templates) - adds citation consistency
- **Increment 3**: US3 (Tier Classification) - adds tier system
- **Increment 4**: US4 (Gap Detection) - adds AUTO-INCREMENT integration
- **Increment 5**: US5 (JIT Refactoring) - refactors main protocol for token compliance

---

## Phase 1: Setup

**Goal**: Initialize project structure and verify prerequisites

**Duration**: 15 minutes

### Tasks

- [X] T001 Create JIT sub-files directory structure at `.prompt-os/core/web-research/`
- [X] T002 Verify research findings complete in `C:\Users\kryst\IdeaProjects\itzamna-prompt-os\specs\003-web-research\research.md`

**Completion Criteria**:
- âœ… Directory `.prompt-os/core/web-research/` exists
- âœ… research.md contains all 5 research findings (token count, JIT architecture, scoring rubric, AUTO-INCREMENT API, SPEC-010 patterns)

---

## Phase 3: User Story 1 - Source Validation with Scoring (Priority: P1)

**Story Goal**: AI agents can validate research sources using objective, reproducible 4-dimension scoring (authority 40%, recency 30%, completeness 20%, relevance 10%) and classify into quality tiers (T1-T5).

**Why this priority**: Core capability - foundation for all other enhancements. Without formal validation rules, source quality assessment remains inconsistent.

**Independent Test**:
1. Provide agent with 10 diverse URLs (official docs, GitHub repos, blogs, personal sites)
2. Agent validates each source using `source-validation-rules.md`
3. **PASS**: Scores consistent within Â±5 points when validation repeated (SC-001)
4. **PASS**: kubernetes.io scores ~95/100 â†’ T1, medium.com old post scores ~28/100 â†’ T5 (per spec acceptance scenarios)
5. **PASS**: Conflict case (official domain but low score) assigns tier based on score precedence, flags conflict

**Acceptance Scenarios** (from spec):
- Scenario 1: kubernetes.io/docs â†’ 95/100 (authority=40, recency=28, completeness=18, relevance=9) â†’ Tier 1
- Scenario 2: medium.com/@random/old-post (2020) â†’ 28/100 (authority=10, recency=0, completeness=10, relevance=8) â†’ Tier 5
- Scenario 3: Official .io domain but score 58 â†’ Assigns Tier 3 (score precedence), flags conflict

### Tasks

- [X] T003 [US1] Create validation rubric framework in `.prompt-os/core/web-research/source-validation-rules.md` (section: Introduction, Purpose, 4-Dimension Overview)
- [X] T004 [P] [US1] Document Authority scoring rules (0-40 points) with thresholds in `.prompt-os/core/web-research/source-validation-rules.md` (section: Authority Dimension)
- [X] T005 [P] [US1] Document Recency scoring rules (0-30 points) with thresholds in `.prompt-os/core/web-research/source-validation-rules.md` (section: Recency Dimension)
- [X] T006 [US1] Document Completeness and Relevance scoring rules (0-20, 0-10 points) in `.prompt-os/core/web-research/source-validation-rules.md` (sections: Completeness, Relevance)
- [X] T007 [US1] Add worked examples (kubernetes.io score=95, medium.com score=28, conflict case score=58) and validation workflow (5 steps) to `.prompt-os/core/web-research/source-validation-rules.md` (sections: Worked Examples, Validation Workflow, Conflict Resolution)

**Task Dependencies**:
- T003 (framework) â†’ T004, T005 (dimension details can be written in parallel)
- T004, T005 â†’ T006 (remaining dimensions)
- T006 â†’ T007 (examples require all dimensions defined)

**Effort Estimate**: 2 hours

**Deliverable**: `.prompt-os/core/web-research/source-validation-rules.md` (~200 lines, ~800 tokens)

**Self-Critique Target**: â‰¥95/100 (SC-003)

**Human Gate**: After T007 complete, show preview for approval before committing

---

## Phase 4: User Story 2 - Citation Templates (Priority: P1)

**Story Goal**: AI agents cite sources consistently using appropriate formats (minimal/standard/detailed) that match research depth and context requirements.

**Why this priority**: Critical for T0-SOURCE-01 compliance. Current SPEC-010 baseline skills use inconsistent formats. This standardizes citation across all skills.

**Independent Test**:
1. Generate 3 test skills with different source profiles:
   - Baseline skill (2 official docs) â†’ Should use minimal format
   - Integration skill (docs + GitHub + Stack Overflow) â†’ Should use standard format
   - Research-heavy skill (controversial topic, mixed tiers) â†’ Should use detailed format
2. **PASS**: Each skill uses correct template per selection guidelines (SC-002: 100% consistency)
3. **PASS**: SPEC-010 baseline skills (5 total) already comply with minimal format (retroactive validation)

**Acceptance Scenarios** (from spec):
- Scenario 1: Baseline skill with 2 official docs â†’ Minimal format (YAML URL array only)
- Scenario 2: Framework integration with mixed sources â†’ Standard format (YAML with type, accessed_date)
- Scenario 3: Research-heavy controversial topic â†’ Detailed format (YAML with tier, reliability_score, notes)

### Tasks

- [X] T008 [US2] Create citation templates framework in `.prompt-os/core/web-research/citation-templates.md` (section: Introduction, Template Overview)
- [X] T009 [P] [US2] Document Minimal format (YAML URL array) with use cases and examples in `.prompt-os/core/web-research/citation-templates.md` (section: Minimal Format)
- [X] T010 [P] [US2] Document Standard format (YAML with url, type, accessed) with use cases and examples in `.prompt-os/core/web-research/citation-templates.md` (section: Standard Format)
- [X] T011 [US2] Document Detailed format (YAML with tier, reliability_score, notes) and selection guidelines (decision tree) in `.prompt-os/core/web-research/citation-templates.md` (sections: Detailed Format, Selection Guidelines, SPEC-010 Migration)

**Task Dependencies**:
- T008 (framework) â†’ T009, T010 (minimal/standard can be written in parallel)
- T009, T010 â†’ T011 (detailed format + selection guidelines require all formats defined)

**Effort Estimate**: 1.5 hours

**Deliverable**: `.prompt-os/core/web-research/citation-templates.md` (~150 lines, ~600 tokens)

**Self-Critique Target**: â‰¥95/100 (SC-003)

**Human Gate**: After T011 complete, show preview for approval before committing

---

## Phase 5: User Story 3 - Tier Classification (Priority: P2)

**Story Goal**: AI agents automatically classify sources into quality tiers (T1-T5) using domain patterns and validation scores, with visual indicators (ðŸŸ¢ðŸ”µðŸŸ¡ðŸŸ ðŸ”´) and confidence levels (âœ“âœ“âœ“, âœ“âœ“, âœ“, âš ).

**Why this priority**: Enables automatic quality assessment and helps agents prioritize high-quality sources. Builds on P1 validation rules from US1.

**Independent Test**:
1. Provide agent with 10 mixed-quality sources (official docs, GitHub repos, blogs, unknown domains)
2. Agent applies tier classification rules from `tier-system.md`
3. **PASS**: kubernetes.io/docs (score 95) â†’ T1 ðŸŸ¢ âœ“âœ“âœ“
4. **PASS**: github.com/kubernetes/examples (score 85) â†’ T2 ðŸ”µ âœ“âœ“
5. **PASS**: .io domain but score 55 â†’ T4 ðŸŸ  âš  (score override), conflict flagged

**Acceptance Scenarios** (from spec):
- Scenario 1: kubernetes.io/docs (score 95) â†’ Tier 1 ðŸŸ¢ confidence âœ“âœ“âœ“
- Scenario 2: github.com/kubernetes/examples (5k stars, score 85) â†’ Tier 2 ðŸ”µ confidence âœ“âœ“
- Scenario 3: .io domain but score <60 â†’ Tier 4 (score-based override), flag for manual review

### Tasks

- [X] T012 [US3] Create tier system framework with T1-T5 definitions and score ranges in `.prompt-os/core/web-research/tier-system.md` (section: Introduction, Tier Definitions)
- [X] T013 [P] [US3] Document domain pattern registry (official domains per technology) extracted from current WEB-RESEARCH.md lines 39-68 in `.prompt-os/core/web-research/tier-system.md` (section: Domain Pattern Registry)
- [X] T014 [P] [US3] Document score-based assignment rules with conflict resolution (score takes precedence) in `.prompt-os/core/web-research/tier-system.md` (section: Score-Based Assignment Rules, Conflict Resolution)
- [X] T015 [US3] Add visual indicators guide (ðŸŸ¢ðŸ”µðŸŸ¡ðŸŸ ðŸ”´, âœ“âœ“âœ“âœ“âœ“âš ) and edge case handling (boundary scores, unknown domains) to `.prompt-os/core/web-research/tier-system.md` (sections: Visual Indicators, Edge Case Handling)

**Task Dependencies**:
- T012 (tier definitions) â†’ T013, T014 (domain registry and scoring rules can be written in parallel)
- T013, T014 â†’ T015 (visual indicators and edge cases require tier system + assignment rules defined)

**Effort Estimate**: 1.5 hours

**Deliverable**: `.prompt-os/core/web-research/tier-system.md` (~200 lines, ~800 tokens)

**Self-Critique Target**: â‰¥95/100 (SC-003)

**Human Gate**: After T015 complete, show preview for approval before committing

---

## Phase 6: User Story 4 - Gap Detection & AUTO-INCREMENT Integration (Priority: P2)

**Story Goal**: System automatically detects source quality gaps (missing Tier 1, outdated sources, insufficient coverage, low reliability) and triggers AUTO-INCREMENT protocol for proactive gap notification and memory registration.

**Why this priority**: Enables system evolution through memory-driven improvement cycles. Critical for v2.1.0 memory architecture goals.

**Independent Test**:
1. Simulate 4 gap scenarios after validation completes:
   - Scenario 1: 0 Tier 1 sources found â†’ Triggers "missing_official_docs" gap
   - Scenario 2: All sources >2 years old â†’ Triggers "outdated_sources" gap
   - Scenario 3: Only 1 source total â†’ Triggers "insufficient_coverage" gap
   - Scenario 4: All sources score <50 â†’ Triggers "low_reliability" gap
2. **PASS**: All 4 scenarios correctly trigger AUTO-INCREMENT (SC-005: 4/4 pass)
3. **PASS**: Gap registered in agent memory table format: `| Date | Gap Type | Topic | Trigger | Resolution | Deferred Until |`
4. **PASS**: Human receives inline conversational prompt with options: "Research more | Defer | Accept limitation"

**Acceptance Scenarios** (from spec):
- Scenario 1: 0 T1 sources â†’ Gap type "missing_official_docs", presents suggestion, registers in MEMORY-MANAGEMENT
- Scenario 2: All sources >2 years â†’ Gap type "outdated_sources", offers "research more | defer | accept limitation"
- Scenario 3: Only 1 source â†’ Gap type "insufficient_coverage", recommends finding 1+ additional sources

### Tasks

- [X] T016 [US4] Create gap detection framework with 4 gap scenario definitions (missing_official_docs, outdated_sources, insufficient_coverage, low_reliability) in `.prompt-os/core/web-research/gap-detection.md` (section: Introduction, 4 Gap Scenarios)
- [X] T017 [US4] Document trigger conditions and validation timing (after all sources validated, before skill generation) in `.prompt-os/core/web-research/gap-detection.md` (section: Trigger Conditions)
- [X] T018 [US4] Document AUTO-INCREMENT integration contract (gap_type, topic, trigger_reason, suggested_action parameters) from research.md findings in `.prompt-os/core/web-research/gap-detection.md` (section: AUTO-INCREMENT Integration)
- [X] T019 [US4] Document memory registration format (markdown table) and human decision workflows (inline conversational prompts) in `.prompt-os/core/web-research/gap-detection.md` (sections: Memory Registration Format, Human Decision Workflows)

**Task Dependencies**:
- T016 (gap scenarios) â†’ T017 (trigger conditions)
- T017 â†’ T018 (AUTO-INCREMENT integration)
- T018 â†’ T019 (memory registration and human workflows)

**Effort Estimate**: 1.5 hours

**Deliverable**: `.prompt-os/core/web-research/gap-detection.md` (~100 lines, ~400 tokens)

**Self-Critique Target**: â‰¥95/100 (SC-003)

**Human Gate**: After T019 complete, show preview for approval before committing

---

## Phase 7: User Story 5 - JIT Refactoring (Priority: P3)

**Story Goal**: Main WEB-RESEARCH.md protocol stays under T0-SIZE-01 limit (1,400 tokens, target <1,100 actual) by extracting detailed sections to JIT sub-files while maintaining high-level workflow and clear references.

**Why this priority**: Technical implementation detail for maintainability. Current protocol already exceeds limit (4,010 tokens). JIT architecture enables compliance without sacrificing content depth.

**Independent Test**:
1. Measure token count of refactored main protocol
2. Verify JIT references load correctly when agent follows workflow
3. **PASS**: Main WEB-RESEARCH.md <1,100 tokens (SC-004: <1,400 actual)
4. **PASS**: Agent can load JIT sub-files on-demand (load `source-validation-rules.md` when validating)
5. **PASS**: Total enhancement fits T0-SIZE-01 distributed across 5 files (main + 4 sub-files)

**Acceptance Scenarios** (from spec):
- Scenario 1: Main protocol reduces to ~285 lines (~1,100 tokens) with 4 JIT sub-files
- Scenario 2: Agent sees high-level guidance with JIT references (e.g., "For detailed validation rules, load `.prompt-os/core/web-research/source-validation-rules.md`")
- Scenario 3: Agent loads specific sub-file on-demand without loading all enhancements

### Tasks

- [X] T020 [US5] Extract detailed sections from current WEB-RESEARCH.md (lines 29-68 tier table â†’ tier-system.md, domain registry â†’ tier-system.md) and update main protocol with JIT references in `.prompt-os/core/WEB-RESEARCH.md` - **COMPLETE** (401â†’190 lines, 2,103â†’1,393 tokens)
- [X] T021 [US5] Add JIT reference sections to main protocol (4 references: validation, citations, tiers, gaps) with clear loading instructions in `.prompt-os/core/WEB-RESEARCH.md` (sections: Source Validation, Citation Formats, Source Tiers, Gap Detection) - **COMPLETE** (4 JIT refs added lines 20-75)
- [X] T022 [US5] Verify token budget compliance (measure main protocol <1,100 tokens, adjust if needed) and generate token count report in `C:\Users\kryst\IdeaProjects\itzamna-prompt-os\specs\003-web-research\token-report.md` - **COMPLETE** (1,393 tokens, within 1,400 limit âœ…)

**Task Dependencies**:
- T020 (extract sections) â†’ T021 (add JIT references)
- T021 â†’ T022 (token verification)

**Effort Estimate**: 1 hour

**Deliverable**: Refactored `.prompt-os/core/WEB-RESEARCH.md` (~275 lines, ~1,100 tokens)

**Self-Critique Target**: â‰¥95/100 (SC-003)

**Human Gate**: After T022 complete, show diff for approval before committing

---

## Phase 8: Polish & Cross-Cutting Concerns

**Goal**: Address cross-cutting concerns, generate documentation, and prepare for deployment

**Duration**: 1.5 hours

### Tasks

- [X] T023 Generate validation report for SPEC-010 baseline skills retroactive testing (citation format compliance, source quality scores) in `C:\Users\kryst\IdeaProjects\itzamna-prompt-os\specs\003-web-research\validation-report.md` - **COMPLETE** (5/5 skills 100% citation compliance, T1 quality avg 98.3/100)

**Completion Criteria**:
- âœ… Validation report documents SPEC-010 citation format compliance (SC-002: target 100%)
- âœ… Source quality scores documented (informational, failures = known issues for v2.3.0)
- âœ… All protocols pass Self-Critique â‰¥95/100 average (SC-003)
- âœ… Memory updated atomically (T0-MEMORY-01 compliance, SC-006)

---

## Commit Strategy

**Branch**: master (or create `003-web-research` branch if preferred)

**Atomic Commits** (one per user story after Human Gate approval):

1. `feat(web-research): add source validation rules (SPEC-003 US1)`
   - Files: `.prompt-os/core/web-research/source-validation-rules.md`

2. `feat(web-research): add citation templates (SPEC-003 US2)`
   - Files: `.prompt-os/core/web-research/citation-templates.md`

3. `feat(web-research): add tier system (SPEC-003 US3)`
   - Files: `.prompt-os/core/web-research/tier-system.md`

4. `feat(web-research): add gap detection integration (SPEC-003 US4)`
   - Files: `.prompt-os/core/web-research/gap-detection.md`

5. `feat(web-research): refactor main protocol with JIT architecture (SPEC-003 US5)`
   - Files: `.prompt-os/core/WEB-RESEARCH.md` (modified)

6. `docs(spec-003): add validation report and token analysis`
   - Files: `specs/003-web-research/validation-report.md`, `specs/003-web-research/token-report.md`

**Memory Update** (T0-MEMORY-01 compliance):
- Update `MEMORY.md` with session summary (artifacts created, success metrics, learned actions)
- Update `memory/opencode-memory.md` with detailed session notes (gaps, rejections, execution details)
- Commit memory updates atomically with final commit

---

## Parallel Execution Examples

### Within User Story 1 (US1)

After T003 completes (validation framework):
```
Parallel execution:
- Agent A: Execute T004 (Authority scoring rules)
- Agent B: Execute T005 (Recency scoring rules)

Both write to different sections of same file, but can be done in parallel if coordinated.
Safer: Execute sequentially to avoid merge conflicts.
```

### Within User Story 2 (US2)

After T008 completes (citation framework):
```
Parallel execution:
- Agent A: Execute T009 (Minimal format)
- Agent B: Execute T010 (Standard format)

Both write to different sections, can be done in parallel.
```

### Within User Story 3 (US3)

After T012 completes (tier framework):
```
Parallel execution:
- Agent A: Execute T013 (Domain pattern registry)
- Agent B: Execute T014 (Score-based assignment rules)

Both write to different sections, can be done in parallel.
```

**Recommendation**: Execute sequentially for safety (avoid markdown merge conflicts). Parallel execution only beneficial with file-locking coordination.

---

## Implementation Strategy

### MVP-First Approach

**Phase 1: MVP (US1 only)**
- Deliverable: `source-validation-rules.md` with 4-dimension scoring
- Value: Objective source quality assessment
- Duration: 2 hours
- Independent test: Validate 10 sources, verify Â±5 consistency

**Phase 2: Citation Consistency (US2)**
- Deliverable: `citation-templates.md` with 3 formats
- Value: Standardized citations across all skills
- Duration: 1.5 hours
- Independent test: Generate 3 test skills, verify correct templates

**Phase 3: Tier System (US3)**
- Deliverable: `tier-system.md` with T1-T5 classification
- Value: Automatic quality tiers with visual indicators
- Duration: 1.5 hours
- Independent test: Classify 10 sources, verify tier assignments

**Phase 4: Gap Detection (US4)**
- Deliverable: `gap-detection.md` with AUTO-INCREMENT integration
- Value: Proactive gap notification and memory-driven evolution
- Duration: 1.5 hours
- Independent test: Simulate 4 gap scenarios, verify triggers

**Phase 5: JIT Compliance (US5)**
- Deliverable: Refactored `WEB-RESEARCH.md` <1,100 tokens
- Value: T0-SIZE-01 compliance, maintainability
- Duration: 1 hour
- Independent test: Measure token count, verify JIT loading

**Phase 6: Polish**
- Deliverable: Validation report, documentation
- Duration: 1.5 hours

### Incremental Delivery

Each phase delivers a working, independently testable increment:
- **Increment 1** (MVP): Source validation works â†’ agents can score sources objectively
- **Increment 2**: Citation templates work â†’ agents can cite sources consistently
- **Increment 3**: Tier system works â†’ agents can classify sources automatically
- **Increment 4**: Gap detection works â†’ system evolves proactively
- **Increment 5**: JIT refactoring works â†’ protocol complies with token limits

User can pause implementation after any increment and still have functional enhancements.

---

## Format Validation

**All tasks follow required checklist format**: âœ…

- âœ… Every task starts with `- [ ]` checkbox
- âœ… Every task has sequential ID (T001-T023)
- âœ… Parallelizable tasks marked with `[P]`
- âœ… User story tasks marked with `[US1]` through `[US5]`
- âœ… Setup/Polish tasks have NO story label
- âœ… Every task includes specific file path
- âœ… Task descriptions are clear and actionable

**Example Verification**:
- âœ… CORRECT: `- [ ] T003 [US1] Create validation rubric framework in .prompt-os/core/web-research/source-validation-rules.md`
- âœ… CORRECT: `- [ ] T004 [P] [US1] Document Authority scoring rules in .prompt-os/core/web-research/source-validation-rules.md`

---

## Success Criteria Checklist

| Criterion | Target | Verification Method | Status |
|-----------|--------|---------------------|--------|
| **SC-001** | Â±5 point consistency | Validate 10 sources twice, compare scores | â³ Test after US1 |
| **SC-002** | 100% citation format consistency | Retroactive validation report (T023) | â³ Test after US2 |
| **SC-003** | â‰¥95/100 Self-Critique average | Human Gate after each sub-file | â³ Test after each phase |
| **SC-004** | Main protocol <1,100 tokens | Token count report (T022) | â³ Test after US5 |
| **SC-005** | 4/4 gap scenarios trigger | Simulate 4 scenarios, verify triggers | â³ Test after US4 |
| **SC-006** | 0 T0-MEMORY-01 violations | Memory update in Phase 8 | â³ Test after Polish |
| **SC-007** | 40% time reduction | Structured validation logs (long-term metric) | ðŸ“Š Post-deployment |
| **SC-008** | 60% incident reduction | Post-creation correction tracking (long-term) | ðŸ“Š Post-deployment |

---

## Risk Mitigation

| Risk | Mitigation | Status |
|------|------------|--------|
| Token budget exceeded despite JIT | Research confirmed 2,900 tokens to extract (Task 0.1) | âœ… Resolved |
| AUTO-INCREMENT API incompatible | Research verified API contract (Task 0.4) | âœ… Resolved |
| Scoring not reproducible | Exact thresholds + Â±5 tolerance (Task 0.3) | âœ… Resolved |
| SPEC-010 retroactive failures | Document as known issues (Q3 clarification) | âœ… Resolved |
| Human rejects sub-file | Iterate based on feedback (Human Gate protocol) | âš ï¸ Active |
| Self-Critique <95 average | Add examples, clarify sections (per SC-003) | âš ï¸ Active |

---

**Tasks Ready**: All 23 tasks defined and executable  
**Next Step**: Begin Phase 1 (Setup) â†’ T001, T002  
**Estimated Completion**: 1.5 days from start

---

*Generated by SpecKit Planning Tool | SPEC-003 WEB-RESEARCH Protocol Enhancement | 2026-02-03*
